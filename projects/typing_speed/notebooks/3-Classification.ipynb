{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Typing Profiling: CyberLab Human Model → Cowrie Scoring\n",
    "\n",
    "This notebook builds a **human interaction model** from CyberLab, then applies it to Cowrie honeypot sessions to estimate how **human-like** each session’s command timing appears.\n",
    "\n",
    "We present three levels of inference:\n",
    "\n",
    "1. **Simple PDF Baseline (Quantile Rule)**\n",
    "   Scores sessions under the CyberLab human probability density function (PDF) and labels them using **CyberLab-derived log-likelihood quantiles**.\n",
    "\n",
    "2. **Primary: Human-Likeness Tail Test (Mahalanobis χ²)**\n",
    "   Uses the global CyberLab Gaussian and converts distance-to-human into a **tail probability** `p_human_tail` (smaller ⇒ less human-like).\n",
    "\n",
    "3. **Secondary: Human-vs-Background Posterior (Gated Mixture)**\n",
    "   Adds a background (bot-like) density model (GMM) and computes a **posterior** `p(human | x)` while still keeping CyberLab as the human anchor.\n",
    "\n",
    "Outputs are written to `./output/` as compact, reusable CSVs.\n"
   ],
   "id": "aa7832e197a701fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T09:14:33.756989Z",
     "start_time": "2025-12-17T09:14:32.765368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy.stats import chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "OUT_DIR = Path(\"output\")\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Inputs\n",
    "CYBERLAB_SESS_PATH = \"output/cyberlab_clustering_features.csv\"\n",
    "COWRIE_LINES_PATH  = \"../../fi_fs/data/processed/Cowrie_Merged_Geo_Enriched_lines.csv\"\n",
    "\n",
    "# Outputs\n",
    "HUMAN_MODEL_NPZ    = OUT_DIR / \"human_gaussian_densities.npz\"\n",
    "COWRIE_DENSITY_CSV = OUT_DIR / \"cowrie_session_density_features.csv\"\n",
    "COWRIE_SCORED_CSV  = OUT_DIR / \"cowrie_session_typing_scored.csv\"\n",
    "COWRIE_A_CSV       = OUT_DIR / \"cowrie_session_typing_humanlikeness_tailtest.csv\"\n",
    "COWRIE_B_CSV       = OUT_DIR / \"cowrie_session_typing_posterior_gatedmixture.csv\"\n",
    "\n",
    "print(\"Paths:\")\n",
    "print(\"  CYBERLAB_SESS_PATH:\", CYBERLAB_SESS_PATH)\n",
    "print(\"  COWRIE_LINES_PATH :\", COWRIE_LINES_PATH)\n",
    "print(\"  HUMAN_MODEL_NPZ   :\", HUMAN_MODEL_NPZ)\n",
    "print(\"  COWRIE_DENSITY_CSV:\", COWRIE_DENSITY_CSV)\n",
    "print(\"  COWRIE_SCORED_CSV :\", COWRIE_SCORED_CSV)\n",
    "print(\"  COWRIE_A_CSV      :\", COWRIE_A_CSV)\n",
    "print(\"  COWRIE_B_CSV      :\", COWRIE_B_CSV)\n"
   ],
   "id": "4be5f93f033bc096",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths:\n",
      "  CYBERLAB_SESS_PATH: output/cyberlab_clustering_features.csv\n",
      "  COWRIE_LINES_PATH : ../../fi_fs/data/processed/Cowrie_Merged_Geo_Enriched_lines.csv\n",
      "  HUMAN_MODEL_NPZ   : output/human_gaussian_densities.npz\n",
      "  COWRIE_DENSITY_CSV: output/cowrie_session_density_features.csv\n",
      "  COWRIE_SCORED_CSV : output/cowrie_session_typing_scored.csv\n",
      "  COWRIE_A_CSV      : output/cowrie_session_typing_humanlikeness_tailtest.csv\n",
      "  COWRIE_B_CSV      : output/cowrie_session_typing_posterior_gatedmixture.csv\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T09:21:07.172067Z",
     "start_time": "2025-12-17T09:21:07.165347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tukey upper fence for honeypot outlier removal\n",
    "def tukey_upper_fence_pos(x: pd.Series) -> float:\n",
    "    s = x[(x > 0) & np.isfinite(x)]\n",
    "    if len(s) < 4:\n",
    "        return np.inf\n",
    "    q1, q3 = np.percentile(s, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    return np.inf if iqr <= 0 else (q3 + 1.5 * iqr)\n",
    "\n",
    "# Gaussian fit utility\n",
    "def fit_gaussian(df: pd.DataFrame, cols, reg=1e-6, min_samples=5):\n",
    "    Z = df[cols].dropna().to_numpy()\n",
    "    n, d = Z.shape\n",
    "    if n < min_samples:\n",
    "        raise ValueError(f\"Need >= {min_samples} samples, got {n}.\")\n",
    "    mu = Z.mean(axis=0)\n",
    "    Sigma = np.atleast_2d(np.cov(Z, rowvar=False)) + reg * np.eye(d)\n",
    "    return mu, Sigma\n",
    "\n",
    "def mvn_logpdf(Z, mu, Sigma):\n",
    "    Z = np.atleast_2d(Z)\n",
    "    mu = np.asarray(mu)\n",
    "    Sigma = np.asarray(Sigma)\n",
    "    d = Z.shape[1]\n",
    "    try:\n",
    "        L = np.linalg.cholesky(Sigma)\n",
    "    except np.linalg.LinAlgError:\n",
    "        L = np.linalg.cholesky(Sigma + 1e-5 * np.eye(d))\n",
    "    diff = (Z - mu)\n",
    "    sol = np.linalg.solve(L, diff.T)\n",
    "    maha = np.sum(sol**2, axis=0)\n",
    "    log_det = 2.0 * np.sum(np.log(np.diag(L)))\n",
    "    log_norm = 0.5 * (d * np.log(2.0 * np.pi) + log_det)\n",
    "    return -log_norm - 0.5 * maha\n",
    "\n",
    "def logsumexp(a, axis=1, keepdims=True):\n",
    "    a = np.asarray(a)\n",
    "    m = np.max(a, axis=axis, keepdims=True)\n",
    "    out = np.log(np.sum(np.exp(a - m), axis=axis, keepdims=True)) + m\n",
    "    return out if keepdims else out.squeeze()\n",
    "\n",
    "# Working with three labels currently\n",
    "# human_like = positively human (high score)\n",
    "# uncertain = close to human threshold, not super confident\n",
    "# non_human_like = negatively human (low score)\n",
    "def three_way_label(values, thr_hi, thr_lo, hi=\"human_like\", mid=\"uncertain\", lo=\"non_human_like\"):\n",
    "    # values high = more human-like\n",
    "    return np.where(values >= thr_hi, hi, np.where(values >= thr_lo, mid, lo))\n",
    "\n",
    "def top_bottom(df, score_col, n=3, min_pairs=None):\n",
    "    x = df.copy()\n",
    "    if min_pairs is not None:\n",
    "        x = x[x[\"n_pairs\"] >= int(min_pairs)].copy()\n",
    "    top = x.sort_values(score_col, ascending=False).head(n).copy()\n",
    "    bot = x.sort_values(score_col, ascending=True).head(n).copy()\n",
    "    top.insert(0, \"rank_group\", \"most_human_like\")\n",
    "    bot.insert(0, \"rank_group\", \"most_non_human_like\")\n",
    "    return pd.concat([top, bot], ignore_index=True)\n"
   ],
   "id": "f6bd0fdaf98ec8ff",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 1 - Build the CyberLab human model (anchor distribution)\n",
    "\n",
    "We fit a global multivariate Gaussian in a **log feature space** derived from:\n",
    "- `mean_spc`, `std_spc`, `median_dt`, `median_ts`\n",
    "\n",
    "This defines the CyberLab **human interaction PDF**.\n"
   ],
   "id": "a81265e9b6a54858"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:30:08.108481Z",
     "start_time": "2025-12-17T14:30:08.083364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_sess = pd.read_csv(CYBERLAB_SESS_PATH)\n",
    "print(\"CyberLab sessions loaded:\", len(df_sess))\n",
    "\n",
    "# Match prior convention: drop SQL injection sessions\n",
    "df_clust = df_sess[df_sess.get(\"sample\", \"\") != \"SQL injection\"].copy()\n",
    "\n",
    "# Reproduce clustering (k=5) so cluster affinities are available later\n",
    "cluster_features = [c for c in [\n",
    "    \"median_ts\",\"median_dt\",\"n_pairs\",\"median_len\",\"mean_spc\",\"std_spc\",\"avg_time_per_char\"\n",
    "] if c in df_clust.columns]\n",
    "\n",
    "if not cluster_features:\n",
    "    raise ValueError(\"No clustering features found in CyberLab CSV.\")\n",
    "\n",
    "X = df_clust[cluster_features].copy()\n",
    "for c in [c for c in [\"n_pairs\",\"median_dt\",\"median_ts\",\"mean_spc\",\"std_spc\",\"avg_time_per_char\"] if c in X.columns]:\n",
    "    X[c] = np.log1p(X[c])\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "df_clust[\"cluster\"] = KMeans(n_clusters=5, random_state=42, n_init=10).fit_predict(X_scaled)\n",
    "\n",
    "print(\"CyberLab cluster counts:\")\n",
    "print(df_clust[\"cluster\"].value_counts().sort_index())\n",
    "\n",
    "# Density feature space (log domain)\n",
    "eps = 1e-6\n",
    "required = [\"mean_spc\",\"std_spc\",\"median_dt\",\"median_ts\"]\n",
    "missing = [c for c in required if c not in df_clust.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"CyberLab missing required columns for density model: {missing}\")\n",
    "\n",
    "df_clust[\"log_mean_spc\"]  = np.log(df_clust[\"mean_spc\"]  + eps)\n",
    "df_clust[\"log_std_spc\"]   = np.log(df_clust[\"std_spc\"]   + eps)\n",
    "df_clust[\"log_median_dt\"] = np.log(df_clust[\"median_dt\"] + eps)\n",
    "df_clust[\"log_median_ts\"] = np.log(df_clust[\"median_ts\"] + eps)\n",
    "\n",
    "density_cols = [\"log_mean_spc\",\"log_std_spc\",\"log_median_dt\",\"log_median_ts\"]\n",
    "df_cy = df_clust.dropna(subset=density_cols).copy()\n",
    "\n",
    "print(\"CyberLab rows with density features:\", len(df_cy), \"/\", len(df_clust))\n",
    "df_cy[density_cols].describe().T.round(3)\n"
   ],
   "id": "5961c6adf62df5ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CyberLab sessions loaded: 166\n",
      "CyberLab cluster counts:\n",
      "cluster\n",
      "0    53\n",
      "1    62\n",
      "2     2\n",
      "3     2\n",
      "4    43\n",
      "Name: count, dtype: int64\n",
      "CyberLab rows with density features: 162 / 162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "               count   mean    std    min    25%    50%    75%    max\n",
       "log_mean_spc   162.0  1.044  0.647 -0.598  0.560  1.066  1.544  2.288\n",
       "log_std_spc    162.0  1.558  0.737 -0.165  1.037  1.610  2.175  3.246\n",
       "log_median_dt  162.0  2.607  1.184 -6.907  2.079  2.697  3.324  5.342\n",
       "log_median_ts  162.0 -0.038  1.157 -2.297 -0.539 -0.138  0.343  9.649"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>log_mean_spc</th>\n",
       "      <td>162.0</td>\n",
       "      <td>1.044</td>\n",
       "      <td>0.647</td>\n",
       "      <td>-0.598</td>\n",
       "      <td>0.560</td>\n",
       "      <td>1.066</td>\n",
       "      <td>1.544</td>\n",
       "      <td>2.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_std_spc</th>\n",
       "      <td>162.0</td>\n",
       "      <td>1.558</td>\n",
       "      <td>0.737</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.610</td>\n",
       "      <td>2.175</td>\n",
       "      <td>3.246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_median_dt</th>\n",
       "      <td>162.0</td>\n",
       "      <td>2.607</td>\n",
       "      <td>1.184</td>\n",
       "      <td>-6.907</td>\n",
       "      <td>2.079</td>\n",
       "      <td>2.697</td>\n",
       "      <td>3.324</td>\n",
       "      <td>5.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_median_ts</th>\n",
       "      <td>162.0</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>1.157</td>\n",
       "      <td>-2.297</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.343</td>\n",
       "      <td>9.649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:36:57.441576Z",
     "start_time": "2025-12-17T14:36:57.430228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mu_h, Sigma_h = fit_gaussian(df_cy, density_cols, reg=1e-6, min_samples=5)\n",
    "print(\"Fitted CyberLab global human Gaussian.\")\n",
    "\n",
    "# Per-cluster Gaussians (human “types”)\n",
    "cluster_gaussians = {}\n",
    "cluster_ids = []\n",
    "for k, df_k in df_cy.groupby(\"cluster\"):\n",
    "    try:\n",
    "        mu_k, Sigma_k = fit_gaussian(df_k, density_cols, reg=1e-6, min_samples=2)\n",
    "        cluster_gaussians[int(k)] = (mu_k, Sigma_k, len(df_k))\n",
    "        cluster_ids.append(int(k))\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "cluster_ids = sorted(cluster_ids)\n",
    "priors = (\n",
    "    df_cy[df_cy[\"cluster\"].isin(cluster_ids)][\"cluster\"]\n",
    "    .value_counts(normalize=True).sort_index().reindex(cluster_ids).to_numpy()\n",
    ")\n",
    "mus = np.stack([cluster_gaussians[cid][0] for cid in cluster_ids])\n",
    "Sigmas = np.stack([cluster_gaussians[cid][1] for cid in cluster_ids])\n",
    "\n",
    "np.savez(\n",
    "    HUMAN_MODEL_NPZ,\n",
    "    feature_cols=np.array(density_cols),\n",
    "    mu_global=mu_h,\n",
    "    Sigma_global=Sigma_h,\n",
    "    cluster_ids=np.array(cluster_ids),\n",
    "    cluster_priors=priors,\n",
    "    mus=mus,\n",
    "    Sigmas=Sigmas,\n",
    ")\n",
    "\n",
    "print(\"Saved model:\", HUMAN_MODEL_NPZ)\n",
    "print(\"cluster_ids:\", cluster_ids)\n",
    "# Gaussian mixture priors represent the proportion of CyberLab sessions in each cluster\n",
    "print(\"priors:\", np.round(priors, 3))\n"
   ],
   "id": "4f1fcf1c0213b03a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted CyberLab global human Gaussian.\n",
      "Saved model: output/human_gaussian_densities.npz\n",
      "cluster_ids: [0, 1, 2, 3, 4]\n",
      "priors: [0.327 0.383 0.012 0.012 0.265]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 2 - Build Cowrie session density features (from line-level logs)\n",
    "\n",
    "Cowrie is line-level. We reconstruct per-session interaction timing by:\n",
    "1) sorting by `session, timestamp`,\n",
    "2) taking inter-command diffs (`time_diff`),\n",
    "3) applying **within-session Tukey** to remove long breaks,\n",
    "4) deriving seconds-per-char (SPC) and typing speed (CPS),\n",
    "5) aggregating to a session-level table.\n",
    "\n",
    "We then create the same **log density features** required by the CyberLab model.\n"
   ],
   "id": "7d186194f1255bb6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:37:58.216024Z",
     "start_time": "2025-12-17T14:37:50.254444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_lines = pd.read_csv(COWRIE_LINES_PATH)\n",
    "df_lines[\"timestamp\"] = pd.to_datetime(df_lines[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "df_lines = df_lines.dropna(subset=[\"timestamp\",\"session\"]).copy()\n",
    "df_lines = df_lines.sort_values([\"session\",\"timestamp\"])\n",
    "\n",
    "df_lines[\"time_diff\"] = df_lines.groupby(\"session\")[\"timestamp\"].diff().dt.total_seconds()\n",
    "df_lines[\"cmd_length\"] = df_lines[\"message\"].astype(str).str.len()\n",
    "\n",
    "df_pairs = df_lines[(df_lines[\"time_diff\"] > 0) & (df_lines[\"cmd_length\"] > 0)].copy()\n",
    "print(\"Pair rows before Tukey:\", len(df_pairs))\n",
    "\n",
    "df_pairs[\"upper_fence\"] = df_pairs.groupby(\"session\")[\"time_diff\"].transform(tukey_upper_fence_pos)\n",
    "df_pairs = df_pairs[df_pairs[\"time_diff\"] <= df_pairs[\"upper_fence\"]].copy()\n",
    "print(\"Pair rows after Tukey :\", len(df_pairs))\n",
    "\n",
    "df_pairs[\"seconds_per_char\"] = df_pairs[\"time_diff\"] / df_pairs[\"cmd_length\"]\n",
    "df_pairs[\"typing_speed\"]     = df_pairs[\"cmd_length\"] / df_pairs[\"time_diff\"]\n",
    "\n",
    "# Session aggregation\n",
    "MIN_PAIRS = 3\n",
    "sess = (\n",
    "    df_pairs.groupby(\"session\", as_index=False)\n",
    "    .agg(\n",
    "        n_pairs=(\"time_diff\",\"size\"),\n",
    "        mean_spc=(\"seconds_per_char\",\"mean\"),\n",
    "        std_spc=(\"seconds_per_char\",\"std\"),\n",
    "        median_dt=(\"time_diff\",\"median\"),\n",
    "        median_ts=(\"typing_speed\",\"median\"),\n",
    "    )\n",
    ")\n",
    "sess = sess[sess[\"n_pairs\"] >= MIN_PAIRS].copy()\n",
    "print(f\"Sessions with >= {MIN_PAIRS} pairs:\", len(sess))\n",
    "\n",
    "# Density features\n",
    "EPS = 1e-9\n",
    "sess[\"log_mean_spc\"]  = np.log(sess[\"mean_spc\"].clip(lower=EPS))\n",
    "sess[\"log_std_spc\"]   = np.log(sess[\"std_spc\"].fillna(sess[\"std_spc\"].median()).clip(lower=EPS))\n",
    "sess[\"log_median_dt\"] = np.log(sess[\"median_dt\"].clip(lower=EPS))\n",
    "sess[\"log_median_ts\"] = np.log(sess[\"median_ts\"].clip(lower=EPS))\n",
    "\n",
    "sess = sess.dropna(subset=density_cols).copy()\n",
    "\n",
    "# Lightweight extra diagnostics\n",
    "extra = (\n",
    "    df_pairs.groupby(\"session\")\n",
    "    .agg(\n",
    "        paste_ratio=(\"time_diff\", lambda s: (s < 0.1).mean()),\n",
    "        mean_dt=(\"time_diff\",\"mean\"),\n",
    "        std_dt=(\"time_diff\",\"std\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "extra[\"dt_cv\"] = extra[\"std_dt\"] / extra[\"mean_dt\"]\n",
    "sess = sess.merge(extra[[\"session\",\"paste_ratio\",\"dt_cv\"]], on=\"session\", how=\"left\")\n",
    "\n",
    "sess.to_csv(COWRIE_DENSITY_CSV, index=False)\n",
    "print(\"Saved:\", COWRIE_DENSITY_CSV)\n",
    "\n",
    "sess.head(5)\n"
   ],
   "id": "7aec58ce8cee4747",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair rows before Tukey: 295901\n",
      "Pair rows after Tukey : 270514\n",
      "Sessions with >= 3 pairs: 49303\n",
      "Saved: output/cowrie_session_density_features.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "        session  n_pairs  mean_spc   std_spc  median_dt     median_ts  \\\n",
       "0  00031aeff1a6        3  0.000181  0.000105   0.001465   4191.114837   \n",
       "1  0003e7887230        3  0.000192  0.000113   0.001505   3986.710963   \n",
       "2  0003f10a2103        3  0.001741  0.002250   0.003836   1564.129301   \n",
       "3  0004025aa9c2       11  0.000126  0.000078   0.012148  12347.711557   \n",
       "4  000cd6e3c127        3  0.000338  0.000160   0.001457   4035.512510   \n",
       "\n",
       "   log_mean_spc  log_std_spc  log_median_dt  log_median_ts  paste_ratio  \\\n",
       "0     -8.619259    -9.158441      -6.525900       8.340722     1.000000   \n",
       "1     -8.555930    -9.086703      -6.498962       8.290722     1.000000   \n",
       "2     -6.353453    -6.096984      -5.563325       7.355085     0.666667   \n",
       "3     -8.978872    -9.459189      -4.410591       9.421226     1.000000   \n",
       "4     -7.993555    -8.740878      -6.531376       8.302889     1.000000   \n",
       "\n",
       "      dt_cv  \n",
       "0  0.671601  \n",
       "1  0.661817  \n",
       "2  1.685798  \n",
       "3  0.484879  \n",
       "4  1.540924  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>n_pairs</th>\n",
       "      <th>mean_spc</th>\n",
       "      <th>std_spc</th>\n",
       "      <th>median_dt</th>\n",
       "      <th>median_ts</th>\n",
       "      <th>log_mean_spc</th>\n",
       "      <th>log_std_spc</th>\n",
       "      <th>log_median_dt</th>\n",
       "      <th>log_median_ts</th>\n",
       "      <th>paste_ratio</th>\n",
       "      <th>dt_cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00031aeff1a6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>4191.114837</td>\n",
       "      <td>-8.619259</td>\n",
       "      <td>-9.158441</td>\n",
       "      <td>-6.525900</td>\n",
       "      <td>8.340722</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.671601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0003e7887230</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>3986.710963</td>\n",
       "      <td>-8.555930</td>\n",
       "      <td>-9.086703</td>\n",
       "      <td>-6.498962</td>\n",
       "      <td>8.290722</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.661817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003f10a2103</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>1564.129301</td>\n",
       "      <td>-6.353453</td>\n",
       "      <td>-6.096984</td>\n",
       "      <td>-5.563325</td>\n",
       "      <td>7.355085</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.685798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0004025aa9c2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.012148</td>\n",
       "      <td>12347.711557</td>\n",
       "      <td>-8.978872</td>\n",
       "      <td>-9.459189</td>\n",
       "      <td>-4.410591</td>\n",
       "      <td>9.421226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.484879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000cd6e3c127</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.001457</td>\n",
       "      <td>4035.512510</td>\n",
       "      <td>-7.993555</td>\n",
       "      <td>-8.740878</td>\n",
       "      <td>-6.531376</td>\n",
       "      <td>8.302889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.540924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b3eb6bff58286fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
